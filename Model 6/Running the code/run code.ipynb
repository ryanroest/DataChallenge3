{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     63\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m     \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pywrap_tensorflow_internal\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m   \u001B[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomRotation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\tensorflow\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrewriter_config_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpywrap_tfe\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpywrap_tensorflow\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pywrap_tfe\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     82\u001B[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001B[1;32m---> 83\u001B[1;33m   \u001B[1;32mraise\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: Traceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-d33c1b041d8d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpreprocessing\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpre\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmeasurement_analysis\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mmea\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mflow_model\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdata_imputation\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mimpu\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpred_to_rain\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mptr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\DataChallenge3\\Model 6\\code\\flow_model.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSequential\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDense\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\DataChallenge3\\lib\\site-packages\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRandomRotation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     raise ImportError(\n\u001B[0m\u001B[0;32m      6\u001B[0m         \u001B[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001B[1;31mImportError\u001B[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# To import the self made functions you need to fill in the path were you \n",
    "# saved the functions instead of the word 'Code'\n",
    "import sys\n",
    "#sys.path.append('C:\\\\Users\\\\s158607\\\\Documents\\\\2019-2020\\\\Data challenge 3\\\\JBG060-Data-Challenge-3-master\\\\code')\n",
    "sys.path.append('C:\\\\Users\\\\s158607\\\\PycharmProjects\\\\DataChallenge3\\\\Model 6\\\\code')\n",
    "\n",
    "import utility\n",
    "import load_files as lf\n",
    "import flow_level_conversion as flc\n",
    "import preprocessing as pre\n",
    "import measurement_analysis as mea\n",
    "import flow_model as model\n",
    "import data_imputation as impu\n",
    "import pred_to_rain as ptr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill the right directories for the data for Aa en Maas in here.\n",
    "PATH = 'C:\\\\Users\\\\20174814\\\\Documents\\\\2019-2020\\\\Data challenge 3\\\\' # CHANGE (!)\n",
    "PATH_MEASUREMENTS = PATH + \"sewer_data\\\\data_pump\\\\RG8150\"\n",
    "PATH_RAIN_DATA = PATH + \"sewer_data\\\\rain_timeseries\"\n",
    "PATH_SHAPE_FILES = PATH + \"sewer_model\\\\aa-en-maas_sewer_shp\"\n",
    "PATH_RAIN_PREDICTION = PATH + \"sewer_data\\\\rain_grid_prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING NECESSARY DATA\n",
    "# Measurements\n",
    "flow_data, level_data = lf.get_measurements(PATH_MEASUREMENTS)\n",
    "flow_data = pre.fill_flow(pre.clean_mes_data(flow_data))\n",
    "level_data = pre.fill_level(pre.clean_mes_data(level_data))\n",
    "\n",
    "# # Actual rain\n",
    "rain_data = lf.get_rain(PATH_RAIN_DATA)\n",
    "\n",
    "# Rain predicton\n",
    "pred_days, rain_prediction = lf.get_rain_prediction(PATH_RAIN_PREDICTION, reduce_grid = True)\n",
    "\n",
    "# Shape file data frames\n",
    "area_data = lf.sdf(PATH_SHAPE_FILES).area_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Value</th>\n",
       "      <th>Rel. Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Theoretical DWF (Q80)</td>\n",
       "      <td>1756.063352</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summer</td>\n",
       "      <td>3658.160068</td>\n",
       "      <td>2.083160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Winter</td>\n",
       "      <td>2576.862376</td>\n",
       "      <td>1.467409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Workday</td>\n",
       "      <td>2999.485208</td>\n",
       "      <td>1.708073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weekend</td>\n",
       "      <td>3195.785694</td>\n",
       "      <td>1.819858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Average</td>\n",
       "      <td>3054.961432</td>\n",
       "      <td>1.739665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name        Value  Rel. Value\n",
       "0  Theoretical DWF (Q80)  1756.063352    1.000000\n",
       "1                 Summer  3658.160068    2.083160\n",
       "2                 Winter  2576.862376    1.467409\n",
       "3                Workday  2999.485208    1.708073\n",
       "4                Weekend  3195.785694    1.819858\n",
       "5                Average  3054.961432    1.739665"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the dwaas haas table\n",
    "area_data.crs = {'init': 'epsg:28992'}\n",
    "measure = mea.measurement_analysis(flow_data, level_data, rain_data, area_data = area_data, village_code = 'DRU')\n",
    "dwaas_table = measure.compare_flow()\n",
    "dwaas_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20174814\\Documents\\2019-2020\\Data challenge 3\\JBG060-Data-Challenge-3-master\\Utility\\data_imputation.py:86: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  flow_values = flow_data.loc[same_level_timestamps]['Value']\n",
      "C:\\Users\\20174814\\Documents\\2019-2020\\Data challenge 3\\JBG060-Data-Challenge-3-master\\Utility\\data_imputation.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  flow_data = flow_data), axis = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          789.470100\n",
       "1          781.623800\n",
       "2          788.569200\n",
       "3          788.704500\n",
       "4          789.410400\n",
       "5          784.205400\n",
       "6          778.151900\n",
       "7          782.990600\n",
       "8          781.058800\n",
       "9          783.236800\n",
       "10         787.941100\n",
       "11         783.785500\n",
       "12         788.164500\n",
       "13         786.169700\n",
       "14         785.905000\n",
       "15         791.113900\n",
       "16         787.861300\n",
       "17         793.535500\n",
       "18         780.900100\n",
       "19         784.984500\n",
       "20         784.238000\n",
       "21         787.854100\n",
       "22         784.290500\n",
       "23         791.661900\n",
       "24         788.677100\n",
       "25         786.770300\n",
       "26         789.513200\n",
       "27         787.225100\n",
       "28         784.494500\n",
       "29         783.263900\n",
       "              ...    \n",
       "3305008    191.735800\n",
       "3305009    184.527700\n",
       "3305010    166.820400\n",
       "3305011    148.522200\n",
       "3305012    130.842200\n",
       "3305013    106.863300\n",
       "3305014     89.076600\n",
       "3305015     71.372180\n",
       "3305016     49.191600\n",
       "3305017     27.238690\n",
       "3305018      6.682343\n",
       "3305019      0.764219\n",
       "3305020      0.347216\n",
       "3305021      0.000000\n",
       "3305022      0.000000\n",
       "3305023      0.000000\n",
       "3305024      0.000000\n",
       "3305025      0.000000\n",
       "3305026      0.000000\n",
       "3305027      0.000000\n",
       "3305028      0.000000\n",
       "3305029      0.000000\n",
       "3305030      0.000000\n",
       "3305031      0.000000\n",
       "3305032      0.000000\n",
       "3305033      0.000000\n",
       "3305034    471.920017\n",
       "3305035    425.920296\n",
       "3305036    501.658309\n",
       "3305037    364.154599\n",
       "Length: 3305038, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the imputated flow\n",
    "imputed_flow = impu.fill_flow(flow_data, level_data)\n",
    "imputed_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12255 samples, validate on 1362 samples\n",
      "Epoch 1/400\n",
      "12255/12255 [==============================] - 1s 42us/step - loss: 48195.1618 - val_loss: 31799.5561\n",
      "Epoch 2/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 43491.0905 - val_loss: 32709.2845\n",
      "Epoch 3/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 43328.5814 - val_loss: 32114.0792\n",
      "Epoch 4/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 42710.1407 - val_loss: 31589.1920\n",
      "Epoch 5/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 42275.4146 - val_loss: 31447.7035\n",
      "Epoch 6/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 41900.2681 - val_loss: 31445.5272\n",
      "Epoch 7/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 41578.4451 - val_loss: 31391.5916\n",
      "Epoch 8/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 41284.8784 - val_loss: 31269.1087\n",
      "Epoch 9/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 41005.1266 - val_loss: 31151.4422\n",
      "Epoch 10/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 40741.5101 - val_loss: 31055.6971\n",
      "Epoch 11/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 40493.9802 - val_loss: 30961.2125\n",
      "Epoch 12/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 40260.4959 - val_loss: 30857.5923\n",
      "Epoch 13/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 40038.6909 - val_loss: 30749.5246\n",
      "Epoch 14/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 39826.9186 - val_loss: 30641.2570\n",
      "Epoch 15/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 39624.0632 - val_loss: 30532.1895\n",
      "Epoch 16/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 39429.1853 - val_loss: 30421.2203\n",
      "Epoch 17/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 39241.3836 - val_loss: 30308.7525\n",
      "Epoch 18/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 39059.9235 - val_loss: 30195.5460\n",
      "Epoch 19/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 38884.1897 - val_loss: 30081.9549\n",
      "Epoch 20/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 38713.7038 - val_loss: 29968.1842\n",
      "Epoch 21/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 38548.0561 - val_loss: 29854.5369\n",
      "Epoch 22/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 38386.9054 - val_loss: 29741.3470\n",
      "Epoch 23/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 38229.9594 - val_loss: 29628.8925\n",
      "Epoch 24/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 38076.9802 - val_loss: 29517.4090\n",
      "Epoch 25/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 37927.7647 - val_loss: 29407.1099\n",
      "Epoch 26/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 37782.1390 - val_loss: 29298.1875\n",
      "Epoch 27/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 37639.9691 - val_loss: 29190.8168\n",
      "Epoch 28/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 37501.0948 - val_loss: 29085.1315\n",
      "Epoch 29/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 37365.4195 - val_loss: 28981.2528\n",
      "Epoch 30/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 37232.8499 - val_loss: 28879.2759\n",
      "Epoch 31/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 37103.2883 - val_loss: 28779.2890\n",
      "Epoch 32/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 36976.6383 - val_loss: 28681.3475\n",
      "Epoch 33/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 36852.8450 - val_loss: 28585.5006\n",
      "Epoch 34/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 36731.8067 - val_loss: 28491.7838\n",
      "Epoch 35/400\n",
      "12255/12255 [==============================] - 0s 36us/step - loss: 36613.4914 - val_loss: 28400.2157\n",
      "Epoch 36/400\n",
      "12255/12255 [==============================] - 0s 35us/step - loss: 36497.8267 - val_loss: 28310.8168\n",
      "Epoch 37/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 36384.7571 - val_loss: 28223.5798\n",
      "Epoch 38/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 36274.2009 - val_loss: 28138.5047\n",
      "Epoch 39/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 36166.1291 - val_loss: 28055.5816\n",
      "Epoch 40/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 36060.4873 - val_loss: 27974.7934\n",
      "Epoch 41/400\n",
      "12255/12255 [==============================] - 0s 36us/step - loss: 35957.2154 - val_loss: 27896.1208\n",
      "Epoch 42/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 35856.2605 - val_loss: 27819.5336\n",
      "Epoch 43/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 35757.5892 - val_loss: 27745.0071\n",
      "Epoch 44/400\n",
      "12255/12255 [==============================] - 1s 45us/step - loss: 35661.1506 - val_loss: 27672.5100\n",
      "Epoch 45/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 35566.8847 - val_loss: 27601.9856\n",
      "Epoch 46/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 35474.7659 - val_loss: 27533.4696\n",
      "Epoch 47/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 35384.7477 - val_loss: 27466.8433\n",
      "Epoch 48/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 35296.7843 - val_loss: 27402.1042\n",
      "Epoch 49/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 35210.8172 - val_loss: 27339.2243\n",
      "Epoch 50/400\n",
      "12255/12255 [==============================] - 0s 35us/step - loss: 35126.8293 - val_loss: 27278.1480\n",
      "Epoch 51/400\n",
      "12255/12255 [==============================] - 0s 39us/step - loss: 35044.7653 - val_loss: 27218.8403\n",
      "Epoch 52/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 34964.5975 - val_loss: 27161.2650\n",
      "Epoch 53/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 34886.2781 - val_loss: 27105.3786\n",
      "Epoch 54/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 34809.7507 - val_loss: 27051.1440\n",
      "Epoch 55/400\n",
      "12255/12255 [==============================] - 0s 36us/step - loss: 34735.0145 - val_loss: 26998.5218\n",
      "Epoch 56/400\n",
      "12255/12255 [==============================] - 1s 45us/step - loss: 34661.9931 - val_loss: 26947.4734\n",
      "Epoch 57/400\n",
      "12255/12255 [==============================] - 1s 49us/step - loss: 34590.6862 - val_loss: 26897.9640\n",
      "Epoch 58/400\n",
      "12255/12255 [==============================] - 0s 39us/step - loss: 34521.0245 - val_loss: 26849.9483\n",
      "Epoch 59/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 34452.9823 - val_loss: 26803.3975\n",
      "Epoch 60/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 34386.5389 - val_loss: 26758.2682\n",
      "Epoch 61/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 34321.6377 - val_loss: 26714.5247\n",
      "Epoch 62/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 34258.2712 - val_loss: 26672.1362\n",
      "Epoch 63/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 34196.3797 - val_loss: 26631.0646\n",
      "Epoch 64/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 34135.9351 - val_loss: 26591.2724\n",
      "Epoch 65/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 34076.9106 - val_loss: 26552.7288\n",
      "Epoch 66/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 34019.2694 - val_loss: 26515.4050\n",
      "Epoch 67/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 33962.9832 - val_loss: 26479.2595\n",
      "Epoch 68/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33908.0201 - val_loss: 26444.2674\n",
      "Epoch 69/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33854.3503 - val_loss: 26410.3945\n",
      "Epoch 70/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 33801.9405 - val_loss: 26377.6101\n",
      "Epoch 71/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 33750.7566 - val_loss: 26345.8853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33700.7739 - val_loss: 26315.1928\n",
      "Epoch 73/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33651.9715 - val_loss: 26285.5018\n",
      "Epoch 74/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33604.3240 - val_loss: 26256.7831\n",
      "Epoch 75/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33557.7886 - val_loss: 26229.0117\n",
      "Epoch 76/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33512.3387 - val_loss: 26202.1608\n",
      "Epoch 77/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33467.9570 - val_loss: 26176.2026\n",
      "Epoch 78/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33424.6274 - val_loss: 26151.1111\n",
      "Epoch 79/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33382.2846 - val_loss: 26126.8668\n",
      "Epoch 80/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 33340.9473 - val_loss: 26103.4386\n",
      "Epoch 81/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 33300.5748 - val_loss: 26080.8049\n",
      "Epoch 82/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 33261.1341 - val_loss: 26058.9432\n",
      "Epoch 83/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33222.6310 - val_loss: 26037.8345\n",
      "Epoch 84/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33185.0009 - val_loss: 26017.4525\n",
      "Epoch 85/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33148.2395 - val_loss: 25997.7732\n",
      "Epoch 86/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33112.3447 - val_loss: 25978.7853\n",
      "Epoch 87/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33077.2603 - val_loss: 25960.4591\n",
      "Epoch 88/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33042.9950 - val_loss: 25942.7751\n",
      "Epoch 89/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 33009.5106 - val_loss: 25925.7221\n",
      "Epoch 90/400\n",
      "12255/12255 [==============================] - 0s 22us/step - loss: 32976.7858 - val_loss: 25909.2712\n",
      "Epoch 91/400\n",
      "12255/12255 [==============================] - 0s 22us/step - loss: 32944.8193 - val_loss: 25893.4155\n",
      "Epoch 92/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32913.5687 - val_loss: 25878.1274\n",
      "Epoch 93/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32883.0367 - val_loss: 25863.3935\n",
      "Epoch 94/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32853.1923 - val_loss: 25849.1943\n",
      "Epoch 95/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32824.0151 - val_loss: 25835.5182\n",
      "Epoch 96/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 32795.4949 - val_loss: 25822.3461\n",
      "Epoch 97/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 32767.6106 - val_loss: 25809.6668\n",
      "Epoch 98/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 32740.3414 - val_loss: 25797.4567\n",
      "Epoch 99/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 32713.6808 - val_loss: 25785.7077\n",
      "Epoch 100/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 32687.6175 - val_loss: 25774.4075\n",
      "Epoch 101/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32662.1172 - val_loss: 25763.5347\n",
      "Epoch 102/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 32637.1642 - val_loss: 25753.0833\n",
      "Epoch 103/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 32612.7839 - val_loss: 25743.0413\n",
      "Epoch 104/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 32588.9110 - val_loss: 25733.3865\n",
      "Epoch 105/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32565.5577 - val_loss: 25724.1166\n",
      "Epoch 106/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 32542.7050 - val_loss: 25715.2135\n",
      "Epoch 107/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32520.3346 - val_loss: 25706.6694\n",
      "Epoch 108/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 32498.4325 - val_loss: 25698.4726\n",
      "Epoch 109/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32477.0013 - val_loss: 25690.6128\n",
      "Epoch 110/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32456.0247 - val_loss: 25683.0798\n",
      "Epoch 111/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32435.4804 - val_loss: 25675.8657\n",
      "Epoch 112/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32415.3749 - val_loss: 25668.9535\n",
      "Epoch 113/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32395.6744 - val_loss: 25662.3403\n",
      "Epoch 114/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32376.3681 - val_loss: 25656.0149\n",
      "Epoch 115/400\n",
      "12255/12255 [==============================] - 0s 22us/step - loss: 32357.4589 - val_loss: 25649.9683\n",
      "Epoch 116/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32338.9444 - val_loss: 25644.1915\n",
      "Epoch 117/400\n",
      "12255/12255 [==============================] - 0s 22us/step - loss: 32320.7831 - val_loss: 25638.6794\n",
      "Epoch 118/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32302.9958 - val_loss: 25633.4219\n",
      "Epoch 119/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32285.5535 - val_loss: 25628.4116\n",
      "Epoch 120/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32268.4624 - val_loss: 25623.6393\n",
      "Epoch 121/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32251.7073 - val_loss: 25619.1015\n",
      "Epoch 122/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32235.2653 - val_loss: 25614.7870\n",
      "Epoch 123/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32219.1559 - val_loss: 25610.6958\n",
      "Epoch 124/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32203.3446 - val_loss: 25606.8143\n",
      "Epoch 125/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32187.8342 - val_loss: 25603.1385\n",
      "Epoch 126/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32172.6211 - val_loss: 25599.6606\n",
      "Epoch 127/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32157.6884 - val_loss: 25596.3807\n",
      "Epoch 128/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32143.0316 - val_loss: 25593.2895\n",
      "Epoch 129/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 32128.6514 - val_loss: 25590.3796\n",
      "Epoch 130/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 32114.5243 - val_loss: 25587.6481\n",
      "Epoch 131/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32100.6554 - val_loss: 25585.0902\n",
      "Epoch 132/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32087.0385 - val_loss: 25582.6985\n",
      "Epoch 133/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32073.6647 - val_loss: 25580.4726\n",
      "Epoch 134/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32060.5272 - val_loss: 25578.4026\n",
      "Epoch 135/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32047.6131 - val_loss: 25576.4877\n",
      "Epoch 136/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 32034.9390 - val_loss: 25574.7214\n",
      "Epoch 137/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32022.4725 - val_loss: 25573.1057\n",
      "Epoch 138/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 32010.2214 - val_loss: 25571.6275\n",
      "Epoch 139/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31998.1769 - val_loss: 25570.2862\n",
      "Epoch 140/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31986.3329 - val_loss: 25569.0795\n",
      "Epoch 141/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31974.6910 - val_loss: 25568.0014\n",
      "Epoch 142/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31963.2424 - val_loss: 25567.0535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31951.9868 - val_loss: 25566.2250\n",
      "Epoch 144/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31940.9052 - val_loss: 25565.5203\n",
      "Epoch 145/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31929.9960 - val_loss: 25564.9301\n",
      "Epoch 146/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31919.2685 - val_loss: 25564.4520\n",
      "Epoch 147/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31908.7208 - val_loss: 25564.0850\n",
      "Epoch 148/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31898.3283 - val_loss: 25563.8281\n",
      "Epoch 149/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31888.0950 - val_loss: 25563.6746\n",
      "Epoch 150/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31878.0273 - val_loss: 25563.6224\n",
      "Epoch 151/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31868.1093 - val_loss: 25563.6692\n",
      "Epoch 152/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31858.3413 - val_loss: 25563.8124\n",
      "Epoch 153/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31848.7230 - val_loss: 25564.0487\n",
      "Epoch 154/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31839.2481 - val_loss: 25564.3805\n",
      "Epoch 155/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31829.9178 - val_loss: 25564.7996\n",
      "Epoch 156/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31820.7132 - val_loss: 25565.3044\n",
      "Epoch 157/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31811.6527 - val_loss: 25565.8926\n",
      "Epoch 158/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31802.7145 - val_loss: 25566.5655\n",
      "Epoch 159/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31793.9090 - val_loss: 25567.3183\n",
      "Epoch 160/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31785.2278 - val_loss: 25568.1491\n",
      "Epoch 161/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31776.6572 - val_loss: 25569.0540\n",
      "Epoch 162/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31768.2282 - val_loss: 25570.0339\n",
      "Epoch 163/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31759.9136 - val_loss: 25571.0898\n",
      "Epoch 164/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31751.6908 - val_loss: 25572.2119\n",
      "Epoch 165/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31743.5937 - val_loss: 25573.4038\n",
      "Epoch 166/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31735.5967 - val_loss: 25574.6629\n",
      "Epoch 167/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31727.7250 - val_loss: 25575.9882\n",
      "Epoch 168/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31719.9430 - val_loss: 25577.3749\n",
      "Epoch 169/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31712.2616 - val_loss: 25578.8225\n",
      "Epoch 170/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31704.6809 - val_loss: 25580.3359\n",
      "Epoch 171/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31697.2072 - val_loss: 25581.9019\n",
      "Epoch 172/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31689.8269 - val_loss: 25583.5318\n",
      "Epoch 173/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31682.5382 - val_loss: 25585.2074\n",
      "Epoch 174/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31675.3568 - val_loss: 25586.9425\n",
      "Epoch 175/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31668.2505 - val_loss: 25588.7335\n",
      "Epoch 176/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31661.2220 - val_loss: 25590.5704\n",
      "Epoch 177/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31654.3042 - val_loss: 25592.4638\n",
      "Epoch 178/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31647.4575 - val_loss: 25594.4025\n",
      "Epoch 179/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31640.6865 - val_loss: 25596.3855\n",
      "Epoch 180/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31634.0147 - val_loss: 25598.4197\n",
      "Epoch 181/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31627.4188 - val_loss: 25600.4967\n",
      "Epoch 182/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31620.8983 - val_loss: 25602.6166\n",
      "Epoch 183/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31614.4532 - val_loss: 25604.7822\n",
      "Epoch 184/400\n",
      "12255/12255 [==============================] - 0s 22us/step - loss: 31608.0862 - val_loss: 25606.9869\n",
      "Epoch 185/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31601.7957 - val_loss: 25609.2368\n",
      "Epoch 186/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31595.5753 - val_loss: 25611.5209\n",
      "Epoch 187/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31589.4313 - val_loss: 25613.8433\n",
      "Epoch 188/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31583.3509 - val_loss: 25616.2019\n",
      "Epoch 189/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31577.3441 - val_loss: 25618.5994\n",
      "Epoch 190/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31571.4060 - val_loss: 25621.0334\n",
      "Epoch 191/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31565.5336 - val_loss: 25623.4991\n",
      "Epoch 192/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31559.7166 - val_loss: 25626.0003\n",
      "Epoch 193/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31553.9713 - val_loss: 25628.5313\n",
      "Epoch 194/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31548.2917 - val_loss: 25631.0943\n",
      "Epoch 195/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31542.6780 - val_loss: 25633.6895\n",
      "Epoch 196/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31537.1212 - val_loss: 25636.3110\n",
      "Epoch 197/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31531.6251 - val_loss: 25638.9671\n",
      "Epoch 198/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31526.1919 - val_loss: 25641.6446\n",
      "Epoch 199/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31520.8101 - val_loss: 25644.3523\n",
      "Epoch 200/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31515.4725 - val_loss: 25647.0877\n",
      "Epoch 201/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31510.2219 - val_loss: 25649.8484\n",
      "Epoch 202/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31504.9973 - val_loss: 25652.6325\n",
      "Epoch 203/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31499.8369 - val_loss: 25655.4390\n",
      "Epoch 204/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31494.7302 - val_loss: 25658.2742\n",
      "Epoch 205/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31489.6911 - val_loss: 25661.1274\n",
      "Epoch 206/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31484.6799 - val_loss: 25664.0010\n",
      "Epoch 207/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31479.7300 - val_loss: 25666.9010\n",
      "Epoch 208/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31474.8354 - val_loss: 25669.8184\n",
      "Epoch 209/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31469.9902 - val_loss: 25672.7553\n",
      "Epoch 210/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31465.1776 - val_loss: 25675.7161\n",
      "Epoch 211/400\n",
      "12255/12255 [==============================] - 0s 35us/step - loss: 31460.4388 - val_loss: 25678.6861\n",
      "Epoch 212/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31455.7272 - val_loss: 25681.6853\n",
      "Epoch 213/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31451.0677 - val_loss: 25684.6932\n",
      "Epoch 214/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31446.4564 - val_loss: 25687.7225\n",
      "Epoch 215/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31441.8812 - val_loss: 25690.7635\n",
      "Epoch 216/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31437.3622 - val_loss: 25693.8245\n",
      "Epoch 217/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31432.8825 - val_loss: 25696.8971\n",
      "Epoch 218/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31428.4382 - val_loss: 25699.9886\n",
      "Epoch 219/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31424.0516 - val_loss: 25703.0919\n",
      "Epoch 220/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31419.7028 - val_loss: 25706.2077\n",
      "Epoch 221/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31415.3818 - val_loss: 25709.3382\n",
      "Epoch 222/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31411.1035 - val_loss: 25712.4749\n",
      "Epoch 223/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31406.8725 - val_loss: 25715.6346\n",
      "Epoch 224/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31402.6834 - val_loss: 25718.8015\n",
      "Epoch 225/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31398.5318 - val_loss: 25721.9737\n",
      "Epoch 226/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31394.4197 - val_loss: 25725.1575\n",
      "Epoch 227/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31390.3349 - val_loss: 25728.3565\n",
      "Epoch 228/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31386.3024 - val_loss: 25731.5598\n",
      "Epoch 229/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31382.2920 - val_loss: 25734.7796\n",
      "Epoch 230/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31378.3321 - val_loss: 25738.0037\n",
      "Epoch 231/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31374.3982 - val_loss: 25741.2376\n",
      "Epoch 232/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31370.5048 - val_loss: 25744.4758\n",
      "Epoch 233/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31366.6396 - val_loss: 25747.7247\n",
      "Epoch 234/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31362.8093 - val_loss: 25750.9804\n",
      "Epoch 235/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31359.0241 - val_loss: 25754.2414\n",
      "Epoch 236/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31355.2507 - val_loss: 25757.5111\n",
      "Epoch 237/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31351.5303 - val_loss: 25760.7842\n",
      "Epoch 238/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31347.8399 - val_loss: 25764.0631\n",
      "Epoch 239/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31344.1761 - val_loss: 25767.3488\n",
      "Epoch 240/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31340.5355 - val_loss: 25770.6413\n",
      "Epoch 241/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31336.9375 - val_loss: 25773.9385\n",
      "Epoch 242/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31333.3709 - val_loss: 25777.2377\n",
      "Epoch 243/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31329.8255 - val_loss: 25780.5403\n",
      "Epoch 244/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31326.3148 - val_loss: 25783.8477\n",
      "Epoch 245/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31322.8286 - val_loss: 25787.1604\n",
      "Epoch 246/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31319.3850 - val_loss: 25790.4760\n",
      "Epoch 247/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31315.9607 - val_loss: 25793.7925\n",
      "Epoch 248/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31312.5707 - val_loss: 25797.1129\n",
      "Epoch 249/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31309.1989 - val_loss: 25800.4356\n",
      "Epoch 250/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31305.8543 - val_loss: 25803.7603\n",
      "Epoch 251/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31302.5385 - val_loss: 25807.0869\n",
      "Epoch 252/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31299.2651 - val_loss: 25810.4144\n",
      "Epoch 253/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31296.0014 - val_loss: 25813.7462\n",
      "Epoch 254/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31292.7794 - val_loss: 25817.0790\n",
      "Epoch 255/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31289.5673 - val_loss: 25820.4097\n",
      "Epoch 256/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31286.3821 - val_loss: 25823.7444\n",
      "Epoch 257/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31283.2277 - val_loss: 25827.0746\n",
      "Epoch 258/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31280.0964 - val_loss: 25830.4105\n",
      "Epoch 259/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31277.0031 - val_loss: 25833.7440\n",
      "Epoch 260/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31273.9067 - val_loss: 25837.0775\n",
      "Epoch 261/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 31270.8393 - val_loss: 25840.4104\n",
      "Epoch 262/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31267.8157 - val_loss: 25843.7443\n",
      "Epoch 263/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31264.7975 - val_loss: 25847.0771\n",
      "Epoch 264/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31261.8125 - val_loss: 25850.4079\n",
      "Epoch 265/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31258.8425 - val_loss: 25853.7387\n",
      "Epoch 266/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31255.8958 - val_loss: 25857.0679\n",
      "Epoch 267/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31252.9743 - val_loss: 25860.3972\n",
      "Epoch 268/400\n",
      "12255/12255 [==============================] - 0s 23us/step - loss: 31250.0788 - val_loss: 25863.7195\n",
      "Epoch 269/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31247.2046 - val_loss: 25867.0481\n",
      "Epoch 270/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31244.3415 - val_loss: 25870.3714\n",
      "Epoch 271/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31241.5141 - val_loss: 25873.6921\n",
      "Epoch 272/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31238.7011 - val_loss: 25877.0108\n",
      "Epoch 273/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31235.9082 - val_loss: 25880.3276\n",
      "Epoch 274/400\n",
      "12255/12255 [==============================] - 0s 24us/step - loss: 31233.1328 - val_loss: 25883.6413\n",
      "Epoch 275/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31230.3743 - val_loss: 25886.9546\n",
      "Epoch 276/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31227.6419 - val_loss: 25890.2648\n",
      "Epoch 277/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31224.9267 - val_loss: 25893.5701\n",
      "Epoch 278/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31222.2288 - val_loss: 25896.8749\n",
      "Epoch 279/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31219.5541 - val_loss: 25900.1753\n",
      "Epoch 280/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31216.8965 - val_loss: 25903.4707\n",
      "Epoch 281/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31214.2693 - val_loss: 25906.7680\n",
      "Epoch 282/400\n",
      "12255/12255 [==============================] - 0s 37us/step - loss: 31211.6374 - val_loss: 25910.0595\n",
      "Epoch 283/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12255/12255 [==============================] - 0s 33us/step - loss: 31209.0373 - val_loss: 25913.3474\n",
      "Epoch 284/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31206.4506 - val_loss: 25916.6324\n",
      "Epoch 285/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31203.8810 - val_loss: 25919.9125\n",
      "Epoch 286/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 31201.3348 - val_loss: 25923.1881\n",
      "Epoch 287/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31198.8011 - val_loss: 25926.4627\n",
      "Epoch 288/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31196.2967 - val_loss: 25929.7339\n",
      "Epoch 289/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31193.7929 - val_loss: 25932.9982\n",
      "Epoch 290/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31191.3157 - val_loss: 25936.2609\n",
      "Epoch 291/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 31188.8558 - val_loss: 25939.5197\n",
      "Epoch 292/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31186.4037 - val_loss: 25942.7712\n",
      "Epoch 293/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31183.9828 - val_loss: 25946.0216\n",
      "Epoch 294/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31181.5595 - val_loss: 25949.2666\n",
      "Epoch 295/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31179.1641 - val_loss: 25952.5096\n",
      "Epoch 296/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31176.7802 - val_loss: 25955.7448\n",
      "Epoch 297/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31174.4075 - val_loss: 25958.9731\n",
      "Epoch 298/400\n",
      "12255/12255 [==============================] - 0s 35us/step - loss: 31172.0565 - val_loss: 25962.2013\n",
      "Epoch 299/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 31169.7161 - val_loss: 25965.4237\n",
      "Epoch 300/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31167.3944 - val_loss: 25968.6406\n",
      "Epoch 301/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31165.0809 - val_loss: 25971.8565\n",
      "Epoch 302/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31162.7929 - val_loss: 25975.0641\n",
      "Epoch 303/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31160.5150 - val_loss: 25978.2667\n",
      "Epoch 304/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31158.2427 - val_loss: 25981.4659\n",
      "Epoch 305/400\n",
      "12255/12255 [==============================] - 0s 35us/step - loss: 31155.9918 - val_loss: 25984.6582\n",
      "Epoch 306/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31153.7584 - val_loss: 25987.8451\n",
      "Epoch 307/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31151.5352 - val_loss: 25991.0300\n",
      "Epoch 308/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31149.3266 - val_loss: 25994.2120\n",
      "Epoch 309/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31147.1279 - val_loss: 25997.3841\n",
      "Epoch 310/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31144.9496 - val_loss: 26000.5533\n",
      "Epoch 311/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31142.7683 - val_loss: 26003.7175\n",
      "Epoch 312/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31140.6132 - val_loss: 26006.8734\n",
      "Epoch 313/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31138.4769 - val_loss: 26010.0263\n",
      "Epoch 314/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31136.3413 - val_loss: 26013.1728\n",
      "Epoch 315/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31134.2296 - val_loss: 26016.3154\n",
      "Epoch 316/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31132.1152 - val_loss: 26019.4536\n",
      "Epoch 317/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31130.0353 - val_loss: 26022.5878\n",
      "Epoch 318/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31127.9538 - val_loss: 26025.7117\n",
      "Epoch 319/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31125.8821 - val_loss: 26028.8317\n",
      "Epoch 320/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31123.8276 - val_loss: 26031.9467\n",
      "Epoch 321/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31121.7812 - val_loss: 26035.0593\n",
      "Epoch 322/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31119.7465 - val_loss: 26038.1630\n",
      "Epoch 323/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31117.7318 - val_loss: 26041.2633\n",
      "Epoch 324/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31115.7266 - val_loss: 26044.3562\n",
      "Epoch 325/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31113.7201 - val_loss: 26047.4408\n",
      "Epoch 326/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31111.7268 - val_loss: 26050.5229\n",
      "Epoch 327/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31109.7560 - val_loss: 26053.6010\n",
      "Epoch 328/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31107.8047 - val_loss: 26056.6704\n",
      "Epoch 329/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31105.8411 - val_loss: 26059.7382\n",
      "Epoch 330/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31103.9052 - val_loss: 26062.7991\n",
      "Epoch 331/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31101.9733 - val_loss: 26065.8512\n",
      "Epoch 332/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31100.0505 - val_loss: 26068.8994\n",
      "Epoch 333/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31098.1447 - val_loss: 26071.9446\n",
      "Epoch 334/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31096.2374 - val_loss: 26074.9810\n",
      "Epoch 335/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31094.3521 - val_loss: 26078.0134\n",
      "Epoch 336/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31092.4721 - val_loss: 26081.0375\n",
      "Epoch 337/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31090.5891 - val_loss: 26084.0567\n",
      "Epoch 338/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 31088.7417 - val_loss: 26087.0739\n",
      "Epoch 339/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 31086.8917 - val_loss: 26090.0813\n",
      "Epoch 340/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31085.0549 - val_loss: 26093.0857\n",
      "Epoch 341/400\n",
      "12255/12255 [==============================] - 0s 37us/step - loss: 31083.2191 - val_loss: 26096.0789\n",
      "Epoch 342/400\n",
      "12255/12255 [==============================] - 1s 45us/step - loss: 31081.3999 - val_loss: 26099.0715\n",
      "Epoch 343/400\n",
      "12255/12255 [==============================] - 1s 49us/step - loss: 31079.5901 - val_loss: 26102.0587\n",
      "Epoch 344/400\n",
      "12255/12255 [==============================] - 0s 40us/step - loss: 31077.7900 - val_loss: 26105.0390\n",
      "Epoch 345/400\n",
      "12255/12255 [==============================] - 0s 37us/step - loss: 31076.0024 - val_loss: 26108.0105\n",
      "Epoch 346/400\n",
      "12255/12255 [==============================] - 0s 38us/step - loss: 31074.2172 - val_loss: 26110.9781\n",
      "Epoch 347/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31072.4409 - val_loss: 26113.9403\n",
      "Epoch 348/400\n",
      "12255/12255 [==============================] - 0s 38us/step - loss: 31070.6759 - val_loss: 26116.8995\n",
      "Epoch 349/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 31068.9211 - val_loss: 26119.8454\n",
      "Epoch 350/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31067.1861 - val_loss: 26122.7943\n",
      "Epoch 351/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31065.4437 - val_loss: 26125.7300\n",
      "Epoch 352/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31063.7169 - val_loss: 26128.6656\n",
      "Epoch 353/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31061.9911 - val_loss: 26131.5929\n",
      "Epoch 354/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31060.2812 - val_loss: 26134.5113\n",
      "Epoch 355/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31058.5814 - val_loss: 26137.4278\n",
      "Epoch 356/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31056.8867 - val_loss: 26140.3403\n",
      "Epoch 357/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31055.2011 - val_loss: 26143.2430\n",
      "Epoch 358/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31053.5229 - val_loss: 26146.1437\n",
      "Epoch 359/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31051.8566 - val_loss: 26149.0317\n",
      "Epoch 360/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31050.1890 - val_loss: 26151.9187\n",
      "Epoch 361/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31048.5336 - val_loss: 26154.7974\n",
      "Epoch 362/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31046.8931 - val_loss: 26157.6721\n",
      "Epoch 363/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31045.2652 - val_loss: 26160.5414\n",
      "Epoch 364/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 31043.6359 - val_loss: 26163.4048\n",
      "Epoch 365/400\n",
      "12255/12255 [==============================] - 0s 33us/step - loss: 31042.0111 - val_loss: 26166.2603\n",
      "Epoch 366/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31040.3985 - val_loss: 26169.1119\n",
      "Epoch 367/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31038.7985 - val_loss: 26171.9577\n",
      "Epoch 368/400\n",
      "12255/12255 [==============================] - 0s 29us/step - loss: 31037.1992 - val_loss: 26174.7960\n",
      "Epoch 369/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31035.6128 - val_loss: 26177.6304\n",
      "Epoch 370/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31034.0318 - val_loss: 26180.4575\n",
      "Epoch 371/400\n",
      "12255/12255 [==============================] - ETA: 0s - loss: 34222.464 - 0s 26us/step - loss: 31032.4453 - val_loss: 26183.2806\n",
      "Epoch 372/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31030.8813 - val_loss: 26186.0959\n",
      "Epoch 373/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31029.3201 - val_loss: 26188.9082\n",
      "Epoch 374/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31027.7708 - val_loss: 26191.7083\n",
      "Epoch 375/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31026.2302 - val_loss: 26194.5093\n",
      "Epoch 376/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31024.6930 - val_loss: 26197.2980\n",
      "Epoch 377/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 31023.1610 - val_loss: 26200.0897\n",
      "Epoch 378/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31021.6224 - val_loss: 26202.8667\n",
      "Epoch 379/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31020.1117 - val_loss: 26205.6417\n",
      "Epoch 380/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 31018.5978 - val_loss: 26208.4107\n",
      "Epoch 381/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31017.0929 - val_loss: 26211.1754\n",
      "Epoch 382/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31015.6099 - val_loss: 26213.9283\n",
      "Epoch 383/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 31014.1207 - val_loss: 26216.6832\n",
      "Epoch 384/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 31012.6270 - val_loss: 26219.4292\n",
      "Epoch 385/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 31011.1494 - val_loss: 26222.1688\n",
      "Epoch 386/400\n",
      "12255/12255 [==============================] - ETA: 0s - loss: 31655.380 - 0s 32us/step - loss: 31009.6845 - val_loss: 26224.9006\n",
      "Epoch 387/400\n",
      "12255/12255 [==============================] - 0s 32us/step - loss: 31008.2190 - val_loss: 26227.6294\n",
      "Epoch 388/400\n",
      "12255/12255 [==============================] - 0s 31us/step - loss: 31006.7627 - val_loss: 26230.3543\n",
      "Epoch 389/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31005.3065 - val_loss: 26233.0723\n",
      "Epoch 390/400\n",
      "12255/12255 [==============================] - 0s 30us/step - loss: 31003.8585 - val_loss: 26235.7834\n",
      "Epoch 391/400\n",
      "12255/12255 [==============================] - 0s 35us/step - loss: 31002.4271 - val_loss: 26238.4857\n",
      "Epoch 392/400\n",
      "12255/12255 [==============================] - 0s 34us/step - loss: 31000.9874 - val_loss: 26241.1861\n",
      "Epoch 393/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 30999.5584 - val_loss: 26243.8785\n",
      "Epoch 394/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 30998.1522 - val_loss: 26246.5671\n",
      "Epoch 395/400\n",
      "12255/12255 [==============================] - 0s 28us/step - loss: 30996.7341 - val_loss: 26249.2478\n",
      "Epoch 396/400\n",
      "12255/12255 [==============================] - 0s 27us/step - loss: 30995.3255 - val_loss: 26251.9255\n",
      "Epoch 397/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 30993.9269 - val_loss: 26254.5964\n",
      "Epoch 398/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 30992.5313 - val_loss: 26257.2609\n",
      "Epoch 399/400\n",
      "12255/12255 [==============================] - 0s 25us/step - loss: 30991.1420 - val_loss: 26259.9185\n",
      "Epoch 400/400\n",
      "12255/12255 [==============================] - 0s 26us/step - loss: 30989.7574 - val_loss: 26262.5707\n"
     ]
    }
   ],
   "source": [
    "# creating the prediction of the hourly flow\n",
    "flow_model = model.flow_model(flow_data, level_data, (pred_days, rain_prediction))\n",
    "cross_validation_score = flow_model.StochasticGradientDescent()\n",
    "cross_validation_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}